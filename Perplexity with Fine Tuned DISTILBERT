####### Imports #######
!pip install datasets
!pip install torch
!pip install transformers
import sqlite3
import csv
import html
import pandas as pd
import numpy as np
import datasets
train_file = "g06q_1k.txt"
!pip install torch
!pip install transformers
import torch, transformers
from transformers import BertTokenizer, BertForMaskedLM, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments
import os
import shutil
import math
from datasets import Dataset, load_dataset

 #### Training Functions ###
def prepare_dataset(texts, tokenizer):
    tokenized_texts = tokenizer(texts, truncation=True, max_length=512, padding=True, return_tensors="pt")
    dataset = Dataset.from_dict(tokenized_texts)
    dataset = dataset.map(add_mlm_labels)
    return dataset

def calculate_perplexity(trainer, dataset):
    eval_results = trainer.evaluate(eval_dataset=dataset)
    return math.exp(eval_results["eval_loss"])

# Load the pre-trained model and tokenizer
from transformers import AutoTokenizer, AutoModelForMaskedLM

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

model = AutoModelForMaskedLM.from_pretrained("bert-base-uncased")


def read_text_file(file_path):
    with open(file_path, 'r', encoding='utf-8') as file:
        text = file.read()
    return text

def tokenize_function(examples):
    return tokenizer(examples['text'], return_special_tokens_mask=True, truncation=True, max_length=512)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForMaskedLM.from_pretrained('bert-base-uncased')

# Prepare the dataset
def load_dataset(file_path, tokenizer):
    dataset = TextDataset(
        tokenizer=tokenizer,
        file_path=file_path,
        block_size=128,
    )
    return dataset

def prepare_dataset(texts):
    dataset = Dataset.from_dict({"text": texts})
    tokenized_dataset = dataset.map(tokenize_function, batched=True, num_proc=4, remove_columns=["text"])
    return tokenized_dataset

#####

train_dataset = load_dataset('text', train = txt_base)['train']
train_dataset = train_dataset.map(tokenize_function, batched=True)
train_dataset = train_dataset.map(add_mlm_labels)
training_args = TrainingArguments(
    output_dir="./bert_finetuned",
    overwrite_output_dir=True,
    num_train_epochs=3,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    save_total_limit=1,
    logging_dir="./logs",
)

# Create the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
)


# Create the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
)

# Train the model
trainer.train()

# Save the fine-tuned model and tokenizer to a local directory
output_dir = "./bert_finetuned"
if not os.path.exists(output_dir):
    os.makedirs(output_dir)
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

# Load the model and tokenizer from the saved directory
model_path = "bert_finetuned"
tokenizer = BertTokenizer.from_pretrained(model_path)
model = BertForMaskedLM.from_pretrained(model_path)


